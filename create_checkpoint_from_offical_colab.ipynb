{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elisim/Informer2020/blob/hf/create_checkpoint_from_offical_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook is used to create a Informer checkpoint for the conversion script in HF transformers. Run all cells, and download the checkpoint that was created. \n",
        "\n",
        "Note: This code is taken from the offical colab demo, without the prediction and visualization"
      ],
      "metadata": {
        "id": "Wftn2KkJHYpj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IM6CZzW_CH0"
      },
      "source": [
        "# Informer Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdaIHYx4_ECL"
      },
      "source": [
        "## Download code and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA_i2gbl-rn-",
        "outputId": "8600a87f-7578-4d95-f8a0-e9521ae696fc"
      },
      "source": [
        "!git clone https://github.com/zhouhaoyi/Informer2020.git\n",
        "!git clone https://github.com/zhouhaoyi/ETDataset.git\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Informer2020'...\n",
            "remote: Enumerating objects: 573, done.\u001b[K\n",
            "remote: Total 573 (delta 0), reused 0 (delta 0), pack-reused 573\u001b[K\n",
            "Receiving objects: 100% (573/573), 6.48 MiB | 17.37 MiB/s, done.\n",
            "Resolving deltas: 100% (328/328), done.\n",
            "Cloning into 'ETDataset'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 187 (delta 25), reused 20 (delta 20), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (187/187), 3.86 MiB | 2.07 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "ETDataset  Informer2020  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5GFng7v7Eq0"
      },
      "source": [
        "import sys\n",
        "if not 'Informer2020' in sys.path:\n",
        "    sys.path += ['Informer2020']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW9TS6jp_YXc"
      },
      "source": [
        "# !pip install -r ./Informer2020/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIjZdN5e_SWe"
      },
      "source": [
        "## Experiments: Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPdt-Kwc_RRZ"
      },
      "source": [
        "from utils.tools import dotdict\n",
        "from exp.exp_informer import Exp_Informer\n",
        "import torch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mx2dnwY9dWi"
      },
      "source": [
        "args = dotdict()\n",
        "\n",
        "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
        "\n",
        "args.data = 'ETTh1' # data\n",
        "args.root_path = './ETDataset/ETT-small/' # root path of data file\n",
        "args.data_path = 'ETTh1.csv' # data file\n",
        "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
        "args.target = 'OT' # target feature in S or MS task\n",
        "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
        "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
        "\n",
        "args.seq_len = 96 # input sequence length of Informer encoder\n",
        "args.label_len = 48 # start token length of Informer decoder\n",
        "args.pred_len = 24 # prediction sequence length\n",
        "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
        "\n",
        "args.enc_in = 7 # encoder input size\n",
        "args.dec_in = 7 # decoder input size\n",
        "args.c_out = 7 # output size\n",
        "args.factor = 5 # probsparse attn factor\n",
        "args.d_model = 512 # dimension of model\n",
        "args.n_heads = 8 # num of heads\n",
        "args.e_layers = 2 # num of encoder layers\n",
        "args.d_layers = 1 # num of decoder layers\n",
        "args.d_ff = 2048 # dimension of fcn in model\n",
        "args.dropout = 0.05 # dropout\n",
        "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
        "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
        "args.activation = 'gelu' # activation\n",
        "args.distil = True # whether to use distilling in encoder\n",
        "args.output_attention = False # whether to output attention in ecoder\n",
        "args.mix = True\n",
        "args.padding = 0\n",
        "args.freq = 'h'\n",
        "\n",
        "args.batch_size = 32 \n",
        "args.learning_rate = 0.0001\n",
        "args.loss = 'mse'\n",
        "args.lradj = 'type1'\n",
        "args.use_amp = False # whether to use automatic mixed precision training\n",
        "\n",
        "args.num_workers = 0\n",
        "args.itr = 1\n",
        "args.train_epochs = 6\n",
        "args.patience = 3\n",
        "args.des = 'exp'\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.gpu = 0\n",
        "\n",
        "args.use_multi_gpu = False\n",
        "args.devices = '0,1,2,3'\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_BCYODAwKl9"
      },
      "source": [
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ','')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53o3pZ809p-a"
      },
      "source": [
        "# Set augments by using data name\n",
        "data_parser = {\n",
        "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "}\n",
        "if args.data in data_parser.keys():\n",
        "    data_info = data_parser[args.data]\n",
        "    args.data_path = data_info['data']\n",
        "    args.target = data_info['T']\n",
        "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ5Q2vyKwSfk"
      },
      "source": [
        "args.detail_freq = args.freq\n",
        "args.freq = args.freq[-1:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywY-umrw-mHO",
        "outputId": "d8cd3113-598c-4aef-cebe-1a4f6664ec23"
      },
      "source": [
        "print('Args in experiment:')\n",
        "print(args)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "{'model': 'informer', 'data': 'ETTh1', 'root_path': './ETDataset/ETT-small/', 'data_path': 'ETTh1.csv', 'features': 'M', 'target': 'OT', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 6, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVHZhRB4-on9"
      },
      "source": [
        "Exp = Exp_Informer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "928tzaA2AA2g",
        "outputId": "fc85c729-23ac-4f71-99b7-612ca38d08c4"
      },
      "source": [
        "for ii in range(args.itr):\n",
        "    # setting record of experiments\n",
        "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
        "                args.seq_len, args.label_len, args.pred_len,\n",
        "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
        "\n",
        "    # set experiments\n",
        "    exp = Exp(args)\n",
        "    \n",
        "    # train\n",
        "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "    exp.train(setting)\n",
        "    \n",
        "    # test\n",
        "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "    exp.test(setting)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 8521\n",
            "val 2857\n",
            "test 2857\n",
            "\titers: 100, epoch: 1 | loss: 0.5061738\n",
            "\tspeed: 0.1546s/iter; left time: 231.4868s\n",
            "\titers: 200, epoch: 1 | loss: 0.3417489\n",
            "\tspeed: 0.0745s/iter; left time: 104.0357s\n",
            "Epoch: 1 cost time: 27.925351858139038\n",
            "Epoch: 1, Steps: 266 | Train Loss: 0.4136932 Vali Loss: 0.6506600 Test Loss: 0.5084816\n",
            "Validation loss decreased (inf --> 0.650660).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2820668\n",
            "\tspeed: 0.1783s/iter; left time: 219.4333s\n",
            "\titers: 200, epoch: 2 | loss: 0.2258470\n",
            "\tspeed: 0.0779s/iter; left time: 88.1203s\n",
            "Epoch: 2 cost time: 20.827848434448242\n",
            "Epoch: 2, Steps: 266 | Train Loss: 0.2566556 Vali Loss: 0.6590195 Test Loss: 0.5885953\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.2184717\n",
            "\tspeed: 0.1827s/iter; left time: 176.3012s\n",
            "\titers: 200, epoch: 3 | loss: 0.2196024\n",
            "\tspeed: 0.0802s/iter; left time: 69.3595s\n",
            "Epoch: 3 cost time: 21.315356016159058\n",
            "Epoch: 3, Steps: 266 | Train Loss: 0.1999153 Vali Loss: 0.6603634 Test Loss: 0.7211351\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1682906\n",
            "\tspeed: 0.1834s/iter; left time: 128.2136s\n",
            "\titers: 200, epoch: 4 | loss: 0.1561021\n",
            "\tspeed: 0.0758s/iter; left time: 45.3758s\n",
            "Epoch: 4 cost time: 20.2805495262146\n",
            "Epoch: 4, Steps: 266 | Train Loss: 0.1712264 Vali Loss: 0.6987594 Test Loss: 0.6638687\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 2857\n",
            "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
            "test shape: (2848, 24, 7) (2848, 24, 7)\n",
            "mse:0.5077683925628662, mae:0.5058870315551758\n"
          ]
        }
      ]
    }
  ]
}